from catboost import CatBoostClassifier
from lightgbm import Booster
import streamlit as st
import pandas as pd
from xgboost import XGBClassifier
import utils


# df = pd.read_csv('data/telco_customer_churn_clean.csv')

st.title('Predicting Customer Churn for Subscription Service')

st.markdown("""
According to several sources ([1](https://ordwaylabs.com/resources/guides/subscription-billing-guide/#:~:text=Subscription%20billing%20is%20a%20key,processed%20through%20subscription%20billing%20platforms.)), nearly 60% of digital companies use subscription services as part of their business model.
For some of the fastest-growing companies like Netflix, Spotify, Microsoft and Google, subscription billing has been key to this expansion.
It should come as no surprise that these same companies are actively building machine learning models to predict which customers are likely to cancel their subscriptions and when.
            
With open-sourced subscription data published by sites like [Kaggle](https://www.kaggle.com/) and [GitHub](https://github.com/), I have trained my own churn-prediction system based on the telecommuncations company, **Telco**, an imaginary subsidiary of IBM.
This applet allows you to predict whether generated customer profiles are likely to churn in the next month. You can select from several optimized models including the best [Gradient Boosting]() model to make a prediction. 
I was inspired by this [article on Bagging, Boosting and Stacking Techniques](https://www.baeldung.com/cs/bagging-boosting-stacking-ml-ensemble-models#:~:text=Boosting%20trains%20multiple%20homogenous%20weak,to%20obtain%20a%20meta%2Dmodel) that discusses the benefits of using XGBoost, CatBoost, and LightGBM gradient boosting algorithms for capturing complex relationships and handling imbalanced data found in our subscriptions dataset.


### How to use:
To get a prediction, select an example subscriber in the "Run inference" tab below. 
Each model will run a prediction to determine the likelihood that the selected subscriber profile is going to churn in the next month.

**Note:** All customer data is generated by aggregating random samples from each feature according to the distribution found in the original subscriptions dataset.
No real private customer data is used in this application or in the original dataset.
            
##### Resources:
- Subscriptions Dataset from IBM: [https://github.com/IBM/telco-customer-churn](https://github.com/IBM/telco-customer-churn-on-icp4d/tree/master/data)
- Source Code from GitHub: [https://github.com/evangabe](https://github.com/evangabe)

            
""")

@st.cache_data
def get_customer_image(name=""):
    image_path = f"images/{name.lower()}.jpeg" if len(name) else "https://thispersondoesnotexist.com/"
    st.sidebar.image(image_path, width=256, output_format="JPEG")
    st.sidebar.caption("*This image is [AI-generated](https://thispersondoesnotexist.com) and does not contain a real person's face.")

def build_a_customer(preset):
    def populate_radio(options=('Yes', 'No'), column=None):
        return {
            "options": options,
            "index": options.index(preset[column])
        }

    # Sidebar Title and Image
    st.sidebar.markdown("## Edit this subscriber's profile:")
    get_customer_image(preset["name"])

    # Lifetime Value (LTV) Section
    st.sidebar.markdown("### Lifetime Value (LTV):")
    
    tenure_range = (0, 100)
    tenure = st.sidebar.slider("Months Subscribed", *tenure_range, value=int(preset['tenure']))
    tenure_binned = utils.bin_value(*tenure_range, tenure)
    
    monthlycharges_range = (0, 50)
    monthlycharges = st.sidebar.slider("Monthly Charges (in USD)", *monthlycharges_range, value=int(preset['MonthlyCharges']))
    monthlycharges_binned = utils.bin_value(*monthlycharges_range, monthlycharges)
    
    totalcharges = monthlycharges * tenure
    totalcharges_binned = "Low"  # Edit this logic later as needed
    
    st.sidebar.markdown(f"Total LTV:\t**${totalcharges:.2f}**")

    # Customer Persona Section
    st.sidebar.markdown("---\n### Customer Persona:")
    
    name = st.sidebar.text_input('Full Name', preset['name'])
    gender = st.sidebar.radio('Gender', **populate_radio(("Male", "Female"), "gender"))
    senior_citizen = st.sidebar.radio('Senior Citizen', **populate_radio(column="SeniorCitizen"))
    partner = st.sidebar.radio('Partner', **populate_radio(column="Partner"))
    dependents = st.sidebar.radio('Dependents', **populate_radio(column="Dependents"))

    # Services Section
    st.sidebar.markdown("---\n### Services:")

    phone_service = st.sidebar.radio('Has Phone Service', **populate_radio(column="PhoneService"))
    multiple_lines = st.sidebar.radio('Has Multiple Phone Lines', **populate_radio(column="MultipleLines"))

    internet_service_options = ('DSL', 'Fiber optic', 'No')
    internet_service_type = st.sidebar.selectbox('Type of Internet Service', **populate_radio(internet_service_options, "InternetService"))
    
    # Internet services details based on the type
    if internet_service_type != 'No':
        online_security = st.sidebar.radio('Has Online Security', **populate_radio(column="OnlineSecurity"))
        online_backup = st.sidebar.radio('Has Online Backup', **populate_radio(column="OnlineBackup"))
        device_protection = st.sidebar.radio('Has Device Protection', **populate_radio(column="DeviceProtection"))
        tech_support = st.sidebar.radio('Has Technical Support', **populate_radio(column="TechSupport"))
        streaming_tv = st.sidebar.radio('Has TV Streaming', **populate_radio(column="StreamingTV"))
        streaming_movies = st.sidebar.radio('Has Movie Streaming', **populate_radio(column="StreamingMovies"))
    else:
        online_security = online_backup = device_protection = tech_support = streaming_tv = streaming_movies = 'No internet service'

    # Payment Details Section
    st.sidebar.markdown("---\n### Payment Details:")
    
    contract_options = ('Month-to-month', 'One year', 'Two year')
    contract = st.sidebar.selectbox('Contract Term', **populate_radio(contract_options, "Contract"))
    
    payment_method_options = ('Bank transfer (automatic)', 'Credit card (automatic)', 'Mailed check', 'Electronic check')
    payment_method = st.sidebar.selectbox('Payment Method', **populate_radio(payment_method_options, "PaymentMethod"))
    
    paperless_billing = st.sidebar.radio('Has Paperless Billing', **populate_radio(column="PaperlessBilling"))

    # Create DataFrame with Subscriber Information
    return pd.DataFrame({
        'gender': [gender],
        'SeniorCitizen': [1 if senior_citizen == 'Yes' else 0],
        'Partner': [partner],
        'Dependents': [dependents],
        'PhoneService': [phone_service],
        'MultipleLines': [multiple_lines],
        'InternetService': [internet_service_type],
        'OnlineSecurity': [online_security],
        'OnlineBackup': [online_backup],
        'DeviceProtection': [device_protection],
        'TechSupport': [tech_support],
        'StreamingTV': [streaming_tv],
        'StreamingMovies': [streaming_movies],
        'Contract': [contract],
        'PaperlessBilling': [paperless_billing],
        'PaymentMethod': [payment_method],
        'tenure_binned': [tenure_binned],
        'MonthlyCharges_binned': [monthlycharges_binned],
        'TotalCharges_binned': [totalcharges_binned],
    })


# To implement basic inference:
# [X] Encode fake_customer categories based on encoding for X_train
# [X] Retrieve model
# [X] Make prediction using predict_proba() on click

# For advanced inference:
# [X] Allow user to select model
# [-] Generate fake customer by sampling features from X_train distribution
# [X] Add fake customer examples

# Create tabs
inference_tab, educational_tab = st.tabs(('Run inference', 'Learn more'))

# Inference tab
inference_tab.header("Inference")

# Example customer select
@st.cache_data
def get_example_customers():
    return pd.read_json("./data/fake_customers.json")
    
example_customers = get_example_customers()
preset = inference_tab.selectbox("Choose an example subscriber", example_customers.columns)

fake_customer_df = build_a_customer(example_customers[preset])

@st.cache_data
def cached_get_model(model_name: str):
    return utils.get_model(model_name)

@st.cache_data
def cached_get_all_models():
    return [utils.get_model("xgboost"), utils.get_model("catboost"), utils.get_model("lightgbm")]

@st.cache_data
def cached_encoder():
    pretrain_df = pd.read_pickle("data/pretrain_decoded.pkl")
    features = list(set(pretrain_df.columns) - set(pretrain_df._get_numeric_data().columns) - set(["Churn"]))
    return utils.get_encoder(pretrain_df, features), features

ordinal_encoder, features = cached_encoder()

@st.cache_data
def cached_predict(df):
    df[features] = ordinal_encoder.transform(df[features])[0]
    models = cached_get_all_models()
    xgboost_result = utils.predict_churn(models[0], df)[0]
    catboost_result = utils.predict_churn(models[1], df)[0]
    lightgbm_result = utils.predict_churn(models[2], df)[0]
    return pd.DataFrame({"XGBoost": f"{xgboost_result*100:.2f}%", "CatBoost": f"{catboost_result*100:.2f}%", "LightGBM": f"{lightgbm_result*100:.2f}%"}, index=["Chance of Churn"])

inference_tab.table(fake_customer_df)
prediction_results = cached_predict(fake_customer_df)
inference_tab.table(prediction_results)

educational_tab.header("Learn More")


# XGBoost Explanation
xgboost_accordion = educational_tab.expander("XGBoost", True)
xgboost_accordion.image("./images/roc_xgboost.png", width=512)
xgboost_accordion.markdown("""
**XGBoost (Extreme Gradient Boosting)** is an efficient and scalable implementation of gradient boosting for decision trees. 
It improves on traditional gradient boosting by using a combination of optimized regularization techniques and scalable parallel computing. 
The model works by constructing a sequence of decision trees, where each tree attempts to correct the errors of the previous ones.
""")

xgboost_accordion.markdown("#### Key Formula")
xgboost_accordion.latex(r"""
\text{Obj}(\theta) = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
""")

xgboost_accordion.markdown("""
Where:
- $l(y_i, \hat{y}_i)$ is the loss function (e.g., mean squared error).
- $\Omega(f_k)$ is the regularization term controlling model complexity.
""")
xgboost_accordion.latex(r"""
\Omega(f_k) = \gamma T + \frac{1}{2} \lambda ||w||^2
""")

xgboost_accordion.markdown("""
- $T$ is the number of leaves in the tree, and $w$ represents leaf weights.

XGBoost also uses second-order Taylor expansion to approximate the loss, which improves convergence speed.
""")

# CatBoost Explanation
# TODO - Add ROC Curve here
catboost_accordion = educational_tab.expander("CatBoost")
catboost_accordion.image("./images/roc_catboost.png", width=512)
catboost_accordion.markdown("""
**CatBoost** is a gradient boosting algorithm designed to handle categorical features more effectively. 
It automates the process of encoding categorical variables and reduces the risk of overfitting by using techniques like ordered boosting, which helps minimize prediction bias.
""")

catboost_accordion.markdown("#### Key Process")
catboost_accordion.latex(r"""
TE(x_i) = \frac{\sum_{j \neq i} y_j}{\sum_{j \neq i} 1}
""")

catboost_accordion.markdown("""
Where:
- $x_i$ is a categorical feature.
- $y_j$ represents the target variable for the corresponding categorical feature $x_j$.

Additionally, CatBoost's **ordered boosting** prevents information leakage by ensuring that each prediction only uses data points that were available in previous steps.
""")

# LightGBM Explanation
lightgbm_accordion = educational_tab.expander("LightGBM")
lightgbm_accordion.image("./images/roc_lightgbm.png", width=512)
lightgbm_accordion.markdown("""
**LightGBM** (Light Gradient Boosting Machine) is another gradient boosting framework designed for high efficiency and performance. 
It improves on traditional models by using techniques such as histogram-based decision tree learning and leaf-wise tree growth.
""")

lightgbm_accordion.markdown("#### Key Formula")
lightgbm_accordion.latex(r"""
\text{Obj}(\theta) = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \lambda ||\theta||^2
""")

lightgbm_accordion.markdown("""
LightGBM constructs decision trees using the **leaf-wise growth strategy**, which grows the leaf with the highest loss reduction, rather than growing level-wise:
""")

lightgbm_accordion.latex(r"""
\Delta \text{loss} = \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} - \left( \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} \right)
""")

lightgbm_accordion.markdown("""
Where:
- $G_L$ and $G_R$ are the gradient sums for the left and right leaves.
""")